
---
## ðŸ§  Deep Learning Basics with MNIST â€“ Summary Outline

### 1. **What Is Deep Learning?**
- Works best for large, complex, or unstructured data (e.g., images, text)
- Inspired by the brain â€” built from layers of *artificial neurons*
- Used when classical ML reaches its limits

---

### 2. **Neural Network Structure**
- **Input Layer** â†’ Receives data (e.g. image pixels)
- **Hidden Layers** â†’ Extract patterns with weights, activations
- **Output Layer** â†’ Final prediction (e.g., class scores)

---

### 3. **Training Process Overview**
- Forward pass â†’ Model predicts
- Loss computed â†’ Measures error
- Backward pass â†’ Gradients calculated
- Optimizer step â†’ Weights updated

**Formula:**  
`W_new = W_old - Î· * âˆ‚E/âˆ‚W`

---

### 4. **Training Variants**
- **SGD**: One sample at a time
- **Batch GD**: All data in one go
- **Mini-Batch GD**: Small groups (most practical)

---

### 5. **Optimizers**
- **Vanilla GD**: Basic updates
- **Momentum**: Adds velocity
- **RMSProp**: Adapts per parameter
- **Adam**: Most popular; combines Momentum + RMSProp

---

### 6. **Loss Functions**
- **MSE / MAE** â†’ Regression
- **Binary CrossEntropy** â†’ Binary classification
- **Categorical CrossEntropy** â†’ Multi-class classification (used for MNIST)

---

### 7. **Normalization**
- Input pixel values scaled:
  - `ToTensor()` â†’ [0, 1]
  - `Normalize((0.5,), (0.5,))` â†’ [-1, 1]
- Keeps data centered and stable during training

---

### 8. **PyTorch Workflow Recap**
- `Dataset` + `DataLoader` â†’ Handle batching/shuffling
- `nn.Module` â†’ Define model architecture
- `CrossEntropyLoss()` â†’ Calculates loss
- `Adam(model.parameters())` â†’ Updates weights
- `model.eval() + torch.no_grad()` â†’ Turns off training features for testing

---

### 9. **Evaluation Metrics**
- **Accuracy** â†’ % of correct predictions
- **Confusion Matrix** â†’ Breakdown of true vs predicted per class
- **Classification Report**:
  - **Precision**: Of predicted class X, how many were correct
  - **Recall**: Of actual class X, how many were found
  - **F1-score**: Balance of precision & recall

---

### 10. **Highlights Youâ€™ve Explored**
- `Flatten()` reshapes input to 1D for linear layers
- MNIST is grayscale â†’ 1 channel (`(1, 28, 28)`)
- Output of model = logits â†’ Softmax applied inside loss
- Evaluation collects predictions â†’ compares with true labels

---

